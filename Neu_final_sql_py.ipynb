{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNZf5YZ14NRDpy8+/INvR4i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mital-mihir/SQLite-with-python-and-json/blob/main/Neu_final_sql_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qeJ_JAQzkstT"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = 'https://www.cnbc.com/finance/'\n",
        "response = requests.get(url)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    webpage = response.content\n",
        "    soup = BeautifulSoup(webpage, 'html.parser')\n",
        "\n",
        "    # Find the news articles on the webpage\n",
        "    articles = soup.find_all('a', class_='Card-title')\n",
        "\n",
        "    for article in articles:\n",
        "        title = article.text.strip()\n",
        "        link = article['href']\n",
        "        print(\"Title:\", title)\n",
        "        print(\"Link:\", link)\n",
        "        print(\"---\")\n",
        "else:\n",
        "    print('Failed to retrieve the webpage.')\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def get_cnbc_news():\n",
        "    url = 'https://www.cnbc.com/'\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    articles = soup.find_all('a', class_='Card-title')\n",
        "    news_list = []\n",
        "\n",
        "    for article in articles:\n",
        "        title = article.text.strip()\n",
        "        link = article['href']\n",
        "        news_list.append({'title': title, 'link': link})\n",
        "\n",
        "    return news_list\n",
        "\n",
        "\n",
        "def get_article_content(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    # Find the HTML elements containing the content and extract the relevant information\n",
        "    # You may need to inspect the HTML structure of the specific website to determine the elements to extract.\n",
        "    # Example: article_content = soup.find('div', class_='article-content').text.strip()\n",
        "    # Replace the above line with the appropriate code for the website you are scraping.\n",
        "\n",
        "    return get_article_content\n",
        "\n",
        "# Get CNBC news\n",
        "cnbc_news = get_cnbc_news()\n",
        "for article in cnbc_news:\n",
        "    print(\"Title:\", article['title'])\n",
        "    print(\"Link:\", article['link'])\n",
        "    # Retrieve the content behind the link\n",
        "    article_content = get_article_content(article['link'])\n",
        "    print(\"Content:\", article_content)\n",
        "    print(\"---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klrG7dYjk2Ei",
        "outputId": "30df5654-5fcc-4934-9cda-f8213ef53e67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title: Stocks making the biggest moves after hours: Gap, Marvell Technology, RH, Ulta and more\n",
            "Link: https://www.cnbc.com/2023/05/25/stocks-moving-big-after-hours-gps-mrvl-rh-and-more.html\n",
            "Content: <function get_article_content at 0x7f8f50a5fd00>\n",
            "---\n",
            "Title: S&P 500 futures are slightly lower as investors watch for debt ceiling updates, inflation data: Live...\n",
            "Link: https://www.cnbc.com/2023/05/25/stock-market-today-live-updates.html\n",
            "Content: <function get_article_content at 0x7f8f50a5fd00>\n",
            "---\n",
            "Title: A.I. cryptocurrencies jump after Nvidia reports booming artificial intelligence demand\n",
            "Link: https://www.cnbc.com/2023/05/25/ai-cryptocurrencies-jump-after-nvidia-reports-booming-demand.html\n",
            "Content: <function get_article_content at 0x7f8f50a5fd00>\n",
            "---\n",
            "Title: How the A.I. explosion could save the market and maybe the economy\n",
            "Link: https://www.cnbc.com/2023/05/25/how-the-ai-explosion-could-save-the-market-and-maybe-the-economy.html\n",
            "Content: <function get_article_content at 0x7f8f50a5fd00>\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### store content in sql database with sqlite from CNBC"
      ],
      "metadata": {
        "id": "Gv73fkzsnxVR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_B1TyubxF3a6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import sqlite3\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "#creating connection with sql database\n",
        "conn = sqlite3.connect('news_articles.db')\n",
        "conn.execute(\"PRAGMA foreign_keys = ON\")\n",
        "c = conn.cursor()\n",
        "\n",
        "#creating table to store news articles\n",
        "c.execute('''CREATE TABLE IF NOT EXISTS articles\n",
        "             (id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "             title TEXT,\n",
        "             link TEXT,\n",
        "             content TEXT)''' )\n",
        "\n",
        "def get_cnbc_news():\n",
        "  url = 'https://www.cnbc.com/'\n",
        "  response = requests.get(url)\n",
        "  soup = BeautifulSoup(response.content,'html.parser')\n",
        "\n",
        "  articles = soup.find_all('div', class_ = 'article.body')\n",
        "  news_list = []\n",
        "\n",
        "  for article in articles:\n",
        "      title = article.text.strip()\n",
        "      relative_link = article['href']\n",
        "      absolute_link = urljoin(url, relative_link)\n",
        "      news_list.append({'title': title, 'link': absolute_link})\n",
        "\n",
        "  return news_list\n",
        "\n",
        "def get_article_content(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content,'html.parser')\n",
        "    article_content = \" \"   ## Initialize the content variable\n",
        "\n",
        "    # Extract the content from the webpage\n",
        "    # Update this part based on the HTML structure of the target website\n",
        "    content_elements = soup.find_all('p')  # Example: Extracting <p> elements\n",
        "\n",
        "    for element in content_elements:\n",
        "        article_content += element.text.strip() + \"\\n\" # concating the extracted content\n",
        "    return article_content\n",
        "\n",
        "print(article_content)\n",
        "# get cnbc news\n",
        "cnbs_news = get_cnbc_news()\n",
        "\n",
        "for article in cnbc_news:\n",
        "    title = article['title']\n",
        "    link = article['link']\n",
        "    # Retrieve the content behind the link\n",
        "    article_content = get_article_content(link)\n",
        "    # Insert the article into the database\n",
        "    try:\n",
        "        c.execute(\"INSERT INTO articles (title, link, content) VALUES (?,?,?)\", (title, link, article_content))\n",
        "        conn.commit()\n",
        "    except sqlite3.Error as e:\n",
        "        print(\"An error occured:\", e)\n",
        "\n",
        "    #closing the database connection\n",
        "conn.close()\n"
      ],
      "metadata": {
        "id": "EfazOfR7nwvK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "375cb815-d130-4651-8071-7d56253a6748"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<function get_article_content at 0x7f8f50a5fd00>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting content from CNN  and saving it in sql db"
      ],
      "metadata": {
        "id": "KM81JUutFtCN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import sqlite3\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "#creating connection with sql database\n",
        "conn = sqlite3.connect('news_articles.db')\n",
        "conn.execute(\"PRAGMA foreign_keys = ON\")\n",
        "c = conn.cursor()\n",
        "\n",
        "#creating table to store news articles\n",
        "c.execute('''CREATE TABLE IF NOT EXISTS articles\n",
        "             (id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "             title TEXT,\n",
        "             link TEXT,\n",
        "             content TEXT)''' )\n",
        "\n",
        "def get_cnn_news():\n",
        "  url = 'https://www.cnn.com/'\n",
        "  response = requests.get(url)\n",
        "  soup = BeautifulSoup(response.content,'html.parser')\n",
        "\n",
        "  articles = soup.find_all('div', class_ = 'article.body')\n",
        "  news_list = []\n",
        "\n",
        "  for article in articles:\n",
        "      title = article.text.strip()\n",
        "      relative_link = article['href']\n",
        "      absolute_link = urljoin(url, relative_link)\n",
        "      news_list.append({'title': title, 'link': absolute_link})\n",
        "\n",
        "  return news_list\n",
        "\n",
        "def get_article_content(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content,'html.parser')\n",
        "    article_content = \" \"   ## Initialize the content variable\n",
        "\n",
        "    # Extract the content from the webpage\n",
        "    # Update this part based on the HTML structure of the target website\n",
        "    content_elements = soup.find_all('p')  # Example: Extracting <p> elements\n",
        "\n",
        "    for element in content_elements:\n",
        "        article_content += element.text.strip() + \"\\n\" # concating the extracted content\n",
        "    return article_content\n",
        "\n",
        "print(article_content)\n",
        "# get cnn news\n",
        "cnn_news = get_cnn_news()\n",
        "\n",
        "for article in cnn_news:\n",
        "    title = article['title']\n",
        "    link = article['link']\n",
        "    # Retrieve the content behind the link\n",
        "    article_content = get_article_content(link)\n",
        "    # Insert the article into the database\n",
        "    try:\n",
        "        c.execute(\"INSERT INTO articles (title, link, content) VALUES (' ',' ',' ')\", (title, link, article_content))\n",
        "        conn.commit()\n",
        "    except sqlite3.Error as e:\n",
        "        print(\"An error occured:\", e)\n",
        "\n",
        "    #closing the database connection\n",
        "conn.close()\n"
      ],
      "metadata": {
        "id": "jvRDwmFfFDpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting content from reuters and saving it in sql db"
      ],
      "metadata": {
        "id": "1TmbZwySLe5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import sqlite3\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "#creating connection with sql database\n",
        "conn = sqlite3.connect('news_articles.db')\n",
        "conn.execute(\"PRAGMA foreign_keys = ON\")\n",
        "c = conn.cursor()\n",
        "\n",
        "#creating table to store news articles\n",
        "c.execute('''CREATE TABLE IF NOT EXISTS articles\n",
        "             (id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "             title TEXT,\n",
        "             link TEXT,\n",
        "             content TEXT)''' )\n",
        "\n",
        "def get_reuters_news():\n",
        "  url = 'https://www.reuters.com/markets/europe'\n",
        "  response = requests.get(url)\n",
        "  soup = BeautifulSoup(response.content,'html.parser')\n",
        "\n",
        "  articles = soup.find_all('div', class_ = 'article.body')\n",
        "  news_list = []\n",
        "\n",
        "  for article in articles:\n",
        "      title = article.text.strip()\n",
        "      relative_link = article['href']\n",
        "      absolute_link = urljoin(url, relative_link)\n",
        "      news_list.append({'title': title, 'link': absolute_link})\n",
        "\n",
        "  return news_list\n",
        "\n",
        "def get_article_content(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content,'html.parser')\n",
        "    article_content = \" \"   ## Initialize the content variable\n",
        "\n",
        "    # Extract the content from the webpage\n",
        "    # Update this part based on the HTML structure of the target website\n",
        "    content_elements = soup.find_all('p')  # Example: Extracting <p> elements\n",
        "\n",
        "    for element in content_elements:\n",
        "        article_content += element.text.strip() + \"\\n\" # concating the extracted content\n",
        "    return article_content\n",
        "\n",
        "print(article_content)\n",
        "# get cnn news\n",
        "reuters_news = get_reuters_news()\n",
        "\n",
        "for article in reuters_news:\n",
        "    title = article['title']\n",
        "    link = article['link']\n",
        "    # Retrieve the content behind the link\n",
        "    article_content = get_article_content(link)\n",
        "    # Insert the article into the database\n",
        "    try:\n",
        "        c.execute(\"INSERT INTO articles (title, link, content) VALUES (' ',' ',' ')\", (title, link, article_content))\n",
        "        conn.commit()\n",
        "    except sqlite3.Error as e:\n",
        "        print(\"An error occured:\", e)\n",
        "\n",
        "    #closing the database connection\n",
        "conn.close()\n"
      ],
      "metadata": {
        "id": "JhmhZAumLeFx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6efa4b8-eb9f-4cf1-cf8b-3462250450db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Credit Cards\n",
            "Loans\n",
            "Banking\n",
            "Mortgages\n",
            "Insurance\n",
            "Credit Monitoring\n",
            "Personal Finance\n",
            "Small Business\n",
            "Taxes\n",
            "Help for Low Credit Scores\n",
            "Investing\n",
            "SELECT\n",
            "All Credit Cards\n",
            "Find the Credit Card for You\n",
            "Best Credit Cards\n",
            "Best Rewards Credit Cards\n",
            "Best Travel Credit Cards\n",
            "Best 0% APR Credit Cards\n",
            "Best Balance Transfer Credit Cards\n",
            "Best Cash Back Credit Cards\n",
            "Best Credit Card Welcome Bonuses\n",
            "Best Credit Cards to Build Credit\n",
            "SELECT\n",
            "All Loans\n",
            "Find the Best Personal Loan for You\n",
            "Best Personal Loans\n",
            "Best Debt Consolidation Loans\n",
            "Best Loans to Refinance Credit Card Debt\n",
            "Best Loans with Fast Funding\n",
            "Best Small Personal Loans\n",
            "Best Large Personal Loans\n",
            "Best Personal Loans to Apply Online\n",
            "Best Student Loan Refinance\n",
            "SELECT\n",
            "All Banking\n",
            "Find the Savings Account for You\n",
            "Best High Yield Savings Accounts\n",
            "Best Big Bank Savings Accounts\n",
            "Best Big Bank Checking Accounts\n",
            "Best No Fee Checking Accounts\n",
            "No Overdraft Fee Checking Accounts\n",
            "Best Checking Account Bonuses\n",
            "Best Money Market Accounts\n",
            "Best CDs\n",
            "Best Credit Unions\n",
            "SELECT\n",
            "All Mortgages\n",
            "Best Mortgages\n",
            "Best Mortgages for Small Down Payment\n",
            "Best Mortgages for No Down Payment\n",
            "Best Mortgages with No Origination Fee\n",
            "Best Mortgages for Average Credit Score\n",
            "Adjustable Rate Mortgages\n",
            "Affording a Mortgage\n",
            "SELECT\n",
            "All Insurance\n",
            "Best Life Insurance\n",
            "Best Homeowners Insurance\n",
            "Best Renters Insurance\n",
            "Best Car Insurance\n",
            "Travel Insurance\n",
            "SELECT\n",
            "All Credit Monitoring\n",
            "Best Credit Monitoring Services\n",
            "Best Identity Theft Protection\n",
            "How to Boost Your Credit Score\n",
            "Credit Repair Services\n",
            "SELECT\n",
            "All Personal Finance\n",
            "Best Budgeting Apps\n",
            "Best Expense Tracker Apps\n",
            "Best Money Transfer Apps\n",
            "Best Resale Apps and Sites\n",
            "Buy Now Pay Later (BNPL) Apps\n",
            "Best Debt Relief\n",
            "SELECT\n",
            "All Small Business\n",
            "Best Small Business Savings Accounts\n",
            "Best Small Business Checking Accounts\n",
            "Best Credit Cards for Small Business\n",
            "Best Small Business Loans\n",
            "Best Tax Software for Small Business\n",
            "SELECT\n",
            "All Taxes\n",
            "Best Tax Software\n",
            "Best Tax Software for Small Businesses\n",
            "Tax Refunds\n",
            "SELECT\n",
            "All Help for Low Credit Scores\n",
            "Best Credit Cards for Bad Credit\n",
            "Best Personal Loans for Bad Credit\n",
            "Best Debt Consolidation Loans for Bad Credit\n",
            "Personal Loans if You Don't Have Credit\n",
            "Best Credit Cards for Building Credit\n",
            "Personal Loans for 580 Credit Score or Lower\n",
            "Personal Loans for 670 Credit Score or Lower\n",
            "Best Mortgages for Bad Credit\n",
            "Best Hardship Loans\n",
            "How to Boost Your Credit Score\n",
            "SELECT\n",
            "All Investing\n",
            "Best IRA Accounts\n",
            "Best Roth IRA Accounts\n",
            "Best Investing Apps\n",
            "Best Free Stock Trading Platforms\n",
            "Best Robo-Advisors\n",
            "Index Funds\n",
            "Mutual Funds\n",
            "ETFs\n",
            "Bonds\n",
            "\n",
            "In this article\n",
            "A blockbuster profit report Wednesday from Nvidia crystallized an important point for both markets and the economy: For better or worse, artificial intelligence is the future.\n",
            "Whether it's personalized shopping, self-driving cars or a broad array of robotics uses for health care, gaming and finance, AI will become a factor in virtually everyone's lives.\n",
            "Nvidia's massive fiscal first-quarter earnings helped quantify the phenomenon as the firm nears an elite cast of tech leaders with $1 trillion market valuations and clear leadership status both on Wall Street and in Silicon Valley.\n",
            "\"AI is real, AI is not a fad and we're only in the early innings,\" said Steve Blitz, chief U.S. economist at TS Lombard. \"Does it change the course of the economy over the next three to six months? Probably not. Does it change the economy over the course of the next three to six years? Absolutely, and in very interesting ways.\"\n",
            "Some of the changes Blitz foresees are reduced demand for foreign labor, a \"point of sale\" effect where coding and creative writing can be done by machines instead of people and a host of other activities that go beyond what appears obvious now.\n",
            "Development of products such as OpenAI's ChatGPT, a chatbot that converses with the user, has helped bring home the potential.\n",
            "\"It's hard for me to overstate the value or the impact of AI, and it is in keeping with my view that this coming decade is all about the broader application of technology beyond what we've seen to date, beyond computers and phones, and that application has tremendous upside,\" Blitz said.\n",
            "For Nvidia, the upside already has been apparent.\n",
            "As if profit of $1.09 a share on revenue of $7.19 billion, both well above Wall Street estimates, wasn't enough, the company guided it was expecting $11 billion in sales for the current quarter, largely driven by its leadership position in the AI chip-supplying business.\n",
            "Shares soared more than 26% higher around midday Thursday and the company's market value surpassed $950 billion.\n",
            "Broader market reaction, however, was underwhelming.\n",
            "While the S&P 500 semiconductor index jumped 11.4%, the broader Nasdaq Composite rose a more muted 1.7%. The S&P 500 was up about 0.9%, while the Dow Jones Industrial Average slipped more than 50 points as investors continued to fret over the debt ceiling negotiations in Washington.\n",
            "At the same time, worries of an economic slowdown persisted — despite his excitement over AI, Blitz still thinks the U.S. is headed for recession — and the lopsided market reaction served as a reminder of a stratified economy in which technological benefits tend to spread slowly.\n",
            "\"The spillover and the benefits that the rest of the economy will derive from AI is a multiyear, multidecade process,\" said Peter Boockvar, chief investment officer at Bleakley Advisory Group. \"Is this an incremental piece to growth or is this now diverting spending from other things because every other part of the economy, outside of spending on travel, leisure and restaurants, doesn't seem to be going that well?\"\n",
            "Boockvar pointed out small-cap stocks, for instance, were losing big Thursday, with the Russell 2000 off about 0.8% in early afternoon trading.\n",
            "That happened even though it seems those companies would benefit from the cost-saving aspects of AI such as the ability to reduce staffing expenses. Nvidia's chief competitor in the chip space, Intel, also was getting slammed, down 6.2% on the session. Quarterly tech earnings overall declined 10.4% heading into this week, according to FactSet, though some of the biggest firms did beat Wall Street's lowered expectations.\n",
            "\"There are some serious holes in the economy that we can't ignore here,\" Boockvar said. \"If the AI craze cools, people will see that the underlying business trends of Microsoft, Google and Amazon are clearly slowing because we all breathe the same economic air.\"\n",
            "AI hasn't been a winner for everyone, either.\n",
            "DataTrek Research looked at nine big AI-related companies that came to market through initial public offerings over the past three years and found their collective valuation is down 74% from their debut levels.\n",
            "The group includes UiPath, Pagaya Technologies and Exscientia. Their stocks have rallied in 2023, up an average 41%, but the seven-largest tech companies, a group that includes Nvidia, have surged an average 58%.\n",
            "\"So far, Big Tech has collectively benefited most from the buzz around gen AI. We think this trend will continue given their ability to leverage their global scale and large competitive moats when utilizing this disruptive technology,\" DataTrek co-founder Nicholas Colas wrote. \"Gen AI may end up making US Big Tech even bigger and more systematically important, rather than allowing upstarts to play the classic role of disruptive innovators.\"\n",
            "Indeed, market veteran Art Cashin noted without the big seven stocks, the S&P 500 would surrender all of its 8% gain this year.\n",
            "\"You know, supposedly, the high tide lifts all boats,\" the director of floor operations for UBS said on CNBC's \"Squawk on the Street.\" \"This is a very selective tide. And I'm not ready to throw out the confetti yet.\"\n",
            "Got a confidential news tip? We want to hear from you.\n",
            "Sign up for free newsletters and get more CNBC delivered to your inbox\n",
            "Get this delivered to your inbox, and more info about our products and services.\n",
            "© 2023 CNBC LLC. All Rights Reserved. A Division of NBCUniversal\n",
            "Data is a real-time snapshot *Data is delayed at least 15 minutes. Global Business and Financial News, Stock Quotes, and Market Data and Analysis.\n",
            "Data also provided by\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### reading live stock news from börse online.de"
      ],
      "metadata": {
        "id": "Jt3p3p-mR4NO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import sqlite3\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "#creating connection with sql database\n",
        "conn = sqlite3.connect('news_articles.db')\n",
        "conn.execute(\"PRAGMA foreign_keys = ON\")\n",
        "c = conn.cursor()\n",
        "\n",
        "#creating table to store news articles\n",
        "c.execute('''CREATE TABLE IF NOT EXISTS articles\n",
        "             (id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "             title TEXT,\n",
        "             link TEXT,\n",
        "             content TEXT)''' )\n",
        "\n",
        "def get_börse_news():\n",
        "  url = 'https://www.boerse-online.de/'\n",
        "  response = requests.get(url)\n",
        "  soup = BeautifulSoup(response.content,'html.parser')\n",
        "\n",
        "  articles = soup.find_all('div', class_ = 'article.body')\n",
        "  news_list = []\n",
        "\n",
        "  for article in articles:\n",
        "      title = article.text.strip()\n",
        "      relative_link = article['href']\n",
        "      absolute_link = urljoin(url, relative_link)\n",
        "      news_list.append({'title': title, 'link': absolute_link})\n",
        "\n",
        "  return news_list\n",
        "\n",
        "def get_article_content(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content,'html.parser')\n",
        "    article_content = \" \"   ## Initialize the content variable\n",
        "\n",
        "    # Extract the content from the webpage\n",
        "    # Update this part based on the HTML structure of the target website\n",
        "    content_elements = soup.find_all('p')  # Example: Extracting <p> elements\n",
        "\n",
        "    for element in content_elements:\n",
        "        article_content += element.text.strip() + \"\\n\" # concating the extracted content\n",
        "    return article_content\n",
        "\n",
        "#print(article_content)\n",
        "# get cnn news\n",
        "börse_news = get_börse_news()\n",
        "\n",
        "for article in börse_news:\n",
        "    title = article['title']\n",
        "    link = article['link']\n",
        "    # Retrieve the content behind the link\n",
        "    article_content = get_article_content(link)\n",
        "    # Insert the article into the database\n",
        "    try:\n",
        "        c.execute(\"INSERT INTO articles (title, link, content) VALUES (' ',' ',' ')\", (title, link, article_content))\n",
        "        conn.commit()\n",
        "    except sqlite3.Error as e:\n",
        "        print(\"An error occured:\", e)\n",
        "\n",
        "    #closing the database connection\n",
        "conn.close()\n",
        "\n",
        "\n",
        "# Save news articles to SQLite database\n",
        "def save_articles_to_sqlite(börse_news, db_file_path):\n",
        "    try:\n",
        "        conn = sqlite3.connect('/content/news_articles.db')\n",
        "        cursor = conn.cursor()\n",
        "\n",
        "        cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS news_articles(\n",
        "              id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "              title TEXT,\n",
        "              summary TEXT,\n",
        "              link TEXT,\n",
        "              source TEXT\n",
        "            )\n",
        "         ''')\n",
        "\n",
        "        for article in articles:\n",
        "            title = article['title']\n",
        "            summary = article['summary']\n",
        "            link = article['link']\n",
        "            source = article['source']\n",
        "\n",
        "            cursor.execute('''  #cursor to itearte over rows of data\n",
        "                INSERT INTO news_articles (title, summary, link, source)\n",
        "                VALUES (?, ?, ?, ?)\n",
        "            ''', (title, summary, link, source))\n",
        "\n",
        "        conn.commit()\n",
        "        conn.close()\n",
        "    except Exception as e:\n",
        "          print(\"Error occured during data insertion:\", e)\n",
        "\n",
        "\n",
        "# Provide the correct database file path\n",
        "db_file_path = '/content/news_articles.db'\n",
        "save_articles_to_sqlite(börse_news, db_file_path)\n",
        "print(börse_news)"
      ],
      "metadata": {
        "id": "o9ozMwkTSIB-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b0c77f7-323f-4ffd-c5a4-67f2925c9756"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error occured during data insertion: 'title'\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "\n",
        "\n",
        "def get_börse_news():\n",
        "  url = 'https://www.boerse-online.de/'\n",
        "  response = requests.get(url)\n",
        "  soup = BeautifulSoup(response.content,'html.parser')\n",
        "\n",
        "  articles = soup.find_all('div', class_ = 'article.body')\n",
        "  news_list = []\n",
        "\n",
        "  for article in articles:\n",
        "      title = article.text.strip()\n",
        "      relative_link = article['href']\n",
        "      absolute_link = urljoin(url, relative_link)\n",
        "      news_list.append({'title': title, 'link': absolute_link})\n",
        "\n",
        "  return news_list\n",
        "\n",
        "def get_article_content(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content,'html.parser')\n",
        "    article_content = \" \"   ## Initialize the content variable\n",
        "\n",
        "    # Extract the content from the webpage\n",
        "    # Update this part based on the HTML structure of the target website\n",
        "    content_elements = soup.find_all('p')  # Example: Extracting <p> elements\n",
        "\n",
        "    for element in content_elements:\n",
        "        article_content += element.text.strip() + \"\\n\" # concating the extracted content\n",
        "    return article_content\n",
        "\n",
        "print(article_content)\n",
        "# get cnn news\n",
        "börse_news = get_börse_news()\n",
        "\n",
        "print(börse_news)\n",
        "\n"
      ],
      "metadata": {
        "id": "2tJt3Vpg9Jug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dt-kHtaOUmEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### get data through csv\n",
        "### import python packages"
      ],
      "metadata": {
        "id": "DfjL8g1Ykhk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from datetime import datetime\n",
        "#import plotly_express as px\n",
        "from plotly.offline import init_notebook_mode\n",
        "init_notebook_mode(connected=True)\n",
        "todays_date = datetime.fromtimestamp(time.time()).strftime(\"%Y-%m-%d\")\n",
        "\n",
        "def plot_stock_data(data,title):\n",
        "    '''function for plotting stock data'''\n",
        "    plot = px.line(data,\n",
        "                        x=\"Date\",\n",
        "                        y=[\"Close\"],\n",
        "                        hover_name=\"Date\",\n",
        "                        line_shape=\"linear\",\n",
        "                        title=title)\n",
        "    return plot\n",
        "\n",
        "print(plot_stock_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "UCQOoLQfknK0",
        "outputId": "9b64f3bb-9418-4b0f-d0f7-5ffa04d2853b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "        <script type=\"text/javascript\">\n",
              "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
              "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
              "        if (typeof require !== 'undefined') {\n",
              "        require.undef(\"plotly\");\n",
              "        requirejs.config({\n",
              "            paths: {\n",
              "                'plotly': ['https://cdn.plot.ly/plotly-2.18.2.min']\n",
              "            }\n",
              "        });\n",
              "        require(['plotly'], function(Plotly) {\n",
              "            window._Plotly = Plotly;\n",
              "        });\n",
              "        }\n",
              "        </script>\n",
              "        "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<function plot_stock_data at 0x7f8f5afc0790>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from datetime import datetime\n",
        "import plotly.express as px\n",
        "from plotly.offline import init_notebook_mode\n",
        "init_notebook_mode(connected=True)\n",
        "\n",
        "todays_date = datetime.fromtimestamp(time.time()).strftime(\"%Y-%m-%d\")\n",
        "\n",
        "def plot_stock_data(data, title):\n",
        "    '''function for plotting stock data'''\n",
        "    plot = px.line(data,\n",
        "                   x=\"Date\",\n",
        "                   y=[\"Close\"],\n",
        "                   hover_name=\"Date\",\n",
        "                   line_shape=\"linear\",\n",
        "                   title=title)\n",
        "    return plot\n",
        "\n",
        "# Example usage\n",
        "data = pd.read_csv(\"/content/ADANIPORTS.csv\")\n",
        "data['Date'] = pd.to_datetime(data['Date'])\n",
        "data = data.sort_values(by='Date')\n",
        "\n",
        "title = \"Stock Data\"\n",
        "fig = plot_stock_data(data, title)\n",
        "\n",
        "# Show the plot\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "yTqYgYATO5Ye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from datetime import datetime\n",
        "import plotly.express as px\n",
        "from plotly.offline import plot\n",
        "\n",
        "todays_date = datetime.fromtimestamp(time.time()).strftime(\"%Y-%m-%d\")\n",
        "\n",
        "def plot_stock_data(data, title):\n",
        "    '''function for plotting stock data'''\n",
        "    plot = px.line(data,\n",
        "                   x=\"Date\",\n",
        "                   y=[\"Close\"],\n",
        "                   hover_name=\"Date\",\n",
        "                   line_shape=\"linear\",\n",
        "                   title=title)\n",
        "    return plot\n",
        "\n",
        "# Example usage\n",
        "data = pd.read_csv(\"/content/ADANIPORTS.csv\")\n",
        "data['Date'] = pd.to_datetime(data['Date'])\n",
        "data = data.sort_values(by='Date')\n",
        "\n",
        "title = \"Stock Data\"\n",
        "fig = plot_stock_data(data, title)\n",
        "\n",
        "# Save the plot as an HTML file\n",
        "output_file = \"stock_plot.html\"\n",
        "plot(fig, filename=output_file)\n",
        "\n",
        "print(f\"Plot saved as {output_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyVjzmmmPY7X",
        "outputId": "10b8fab7-f980-4dc9-d592-ce6246e1fb4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plot saved as stock_plot.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load all_stock_5yr market data into your notebook\n"
      ],
      "metadata": {
        "id": "3S9q0xzMnKFt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def get_stock_data(csv_file):\n",
        "    stock_data = pd.read_csv(csv_file)\n",
        "    return stock_data\n",
        "\n",
        "# Example usage\n",
        "csv_file_path = 'stock_data.csv'  # Replace with your file path\n",
        "stocks = get_stock_data('/content/all_stocks_5yr.csv')\n",
        "\n",
        "# Print the stock data\n",
        "print(stocks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQLbnYhmw1pV",
        "outputId": "711f8ce2-6194-474b-fdcb-4061e89c67bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             date    open     high      low   close      volume Name\n",
            "0      2013-02-08   15.07   15.120   14.630   14.75   8407500.0  AAL\n",
            "1      2013-02-11   14.89   15.010   14.260   14.46   8882000.0  AAL\n",
            "2      2013-02-12   14.45   14.510   14.100   14.27   8126000.0  AAL\n",
            "3      2013-02-13   14.30   14.940   14.250   14.66  10259500.0  AAL\n",
            "4      2013-02-14   14.94   14.960   13.160   13.99  31879900.0  AAL\n",
            "...           ...     ...      ...      ...     ...         ...  ...\n",
            "43767  2017-09-13  178.27  178.530  176.745  177.25    231520.0  AMG\n",
            "43768  2017-09-14  177.21  178.630  175.780  178.57    327088.0  AMG\n",
            "43769  2017-09-15  178.43  180.455  178.430  179.87    385790.0  AMG\n",
            "43770  2017-09-18  180.01  182.780  180.010  182.61    401485.0  AMG\n",
            "43771  2017-09-19  182.90  182.900  181.010     NaN         NaN  NaN\n",
            "\n",
            "[43772 rows x 7 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import sqlite3\n",
        "\n",
        "def get_stock_data(csv_file):\n",
        "    stock_data = pd.read_csv(\"all_stocks_5yr.csv\")\n",
        "    return stock_data\n",
        "\n",
        "\n",
        "def save_stock_data_to_sqlite(stock_data, db_file):\n",
        "    # Establish a connection to the SQLite database\n",
        "    conn = sqlite3.connect(db_file)\n",
        "\n",
        "    # Save the stock data to the database\n",
        "    stock_data.to_sql('stocks', conn, if_exists='replace', index=False)\n",
        "\n",
        "    # Close the database connection\n",
        "    conn.close()\n",
        "\n",
        "# Example usage\n",
        "csv_file_path = '/content/all_stocks_5yr.csv'  # Replace with your file path\n",
        "db_file_path = 'stock_data.db'  # Replace with your desired database file path\n",
        "\n",
        "# Get stock data from the CSV file\n",
        "stocks = get_stock_data(csv_file_path)\n",
        "\n",
        "# Save the stock data to SQLite database\n",
        "save_stock_data_to_sqlite(stocks, db_file_path)"
      ],
      "metadata": {
        "id": "mNwoO2gVzqH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### reading nasdaq data"
      ],
      "metadata": {
        "id": "mXSGPXDI2tVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stock_data(csv_file):\n",
        "    stock_data = pd.read_csv(\"nasdaqtraded_full.csv\")\n",
        "    return stock_data\n",
        "\n",
        "csv_file_path = '/nasdaqtraded_full.csv'\n",
        "db_file_path = 'stock_data.db'"
      ],
      "metadata": {
        "id": "3lUmRNqm2hy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### reading adaniports data"
      ],
      "metadata": {
        "id": "0ZD_kyoc2yBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stock_data(csv_file):\n",
        "    stock_data = pd.read_csv(\"ADANIPORTS.csv\")\n",
        "    return stock_data\n",
        "\n",
        "csv_file_path = '/ADANIPORTS.csv'\n",
        "db_file_path = 'stock_data.db'"
      ],
      "metadata": {
        "id": "w04bTcqEP2hd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### reading Deutsche bank equity"
      ],
      "metadata": {
        "id": "KQ1rdKoV3QfG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stock_data(csv_file):\n",
        "    stock_data = pd.read_csv(\"DB.csv\",\"DBK.DE.csv\",\"DBK.F.csv\")\n",
        "    return stock_data\n",
        "\n",
        "csv_file_path = '/DB.csv'\n",
        "csv_file_path = '/DBK.DE.csv'\n",
        "csv_file_path = '/DBK.F.csv'\n",
        "db_file_path = 'stock_data.db'"
      ],
      "metadata": {
        "id": "d7J_j8G2A-HP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reading Tesla stock"
      ],
      "metadata": {
        "id": "Yk6Nkko-Bgmi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stock_data(csv_file):\n",
        "    stock_data = pd.read_csv(\"Tesla_stock_Price\",\"TSLA.csv\")\n",
        "    return stock_data\n",
        "\n",
        "csv_file_path = '/TSLA.csv'\n",
        "csv_file_path = '/Tesla_stock_Price.csv'\n",
        "db_file_path = 'stock_data.db'"
      ],
      "metadata": {
        "id": "hr59MSUHBgKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reading *DOWJONES* stock data"
      ],
      "metadata": {
        "id": "rvorERcAB_qd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stock_data(csv_file):\n",
        "    stock_data = pd.read_csv(\"Tesla_stock_PriceDow Jones Industrial Average Historical Data.csv\")\n",
        "    return stock_data\n",
        "\n",
        "csv_file_path = '/Dow Jones Industrial Average Historical Data.csv'\n",
        "db_file_path = 'stock_data.db'"
      ],
      "metadata": {
        "id": "aBL9MJ2qCEBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reading AAPL,AMZN,GOOG,MSFT data csv file"
      ],
      "metadata": {
        "id": "QghS_wKNlMFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stock_data(csv_file):\n",
        "    stock_data = pd.read_csv(\"Tesla_stock_PriceDow Jones Industrial Average Historical Data.csv\",\"Botcoin.csv\",\"AAPL_data.csv\",\"AMZN_data.csv\",\"GOOG_data.csv\",\"MSFT_data\")\n",
        "    return stock_data\n",
        "\n",
        "csv_file_path = '/Dow Jones Industrial Average Historical Data.csv'\n",
        "csv_file_path = '/content/AAPL_data.csv'\n",
        "csv_file_path = '/content/AMZN_data.csv'\n",
        "csv_file_path = '/content/GOOG_data.csv'\n",
        "csv_file_path = '/content/MSFT_data.csv'\n",
        "csv_file_path = '/content/Bitcoin.csv'\n",
        "\n",
        "db_file_path = 'stock_data.db'"
      ],
      "metadata": {
        "id": "cpW7G3dwlLTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reading data through csv file with pandas"
      ],
      "metadata": {
        "id": "BPiRy_6VG9Wd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import sqlite3\n",
        "from datetime import datetime\n",
        "\n",
        "#data = pd.read_csv(r\"/Tesla_stock_Price.csv\")\n",
        "data = pd.read_csv(r\"/content/ADANIPORTS.csv\")\n",
        "data['Date'] = data['Date'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d'))\n",
        "\n",
        "print(data)\n",
        "\n",
        "def get_stock_data(csv_file):\n",
        "    stock_data = []\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(csv_file)\n",
        "\n",
        "        for index, row in df.iterrows():\n",
        "            date = datetime.strptime(row['Date'], '%Y-%m-%d')\n",
        "            open_price = float(row['Open']) if not pd.isnull(row['Open']) else None\n",
        "            high_price = float(row['High']) if not pd.isnull(row['High']) else None\n",
        "            low_price = float(row['Low']) if not pd.isnull(row['Low']) else None\n",
        "            close_price = float(row['Close']) if not pd.isnull(row['Close']) else None\n",
        "            volume = int(row['Volume']) if not pd.isnull(row['Volume']) else None\n",
        "\n",
        "            stock = {\n",
        "                'date': date,\n",
        "                'open': open_price,\n",
        "                'high': high_price,\n",
        "                'low': low_price,\n",
        "                'close': close_price,\n",
        "                'volume': volume\n",
        "            }\n",
        "\n",
        "            stock_data.append(stock)\n",
        "\n",
        "    except (FileNotFoundError, pd.errors.EmptyDataError) as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "    return stock_data\n",
        "\n",
        "def save_stock_data_to_sqlite(stock_data, db_file):\n",
        "    conn = sqlite3.connect(db_file)\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    # Create table if it doesn't exist\n",
        "    create_table_query = \"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS stocks (\n",
        "        date TEXT,\n",
        "        open REAL,\n",
        "        high REAL,\n",
        "        low REAL,\n",
        "        close REAL,\n",
        "        volume INTEGER\n",
        "    )\n",
        "    \"\"\"\n",
        "    cursor.execute(create_table_query)\n",
        "\n",
        "    # Insert stock data into the table\n",
        "    insert_query = \"\"\"\n",
        "    INSERT INTO stocks (date, open, high, low, close, volume)\n",
        "    VALUES (?, ?, ?, ?, ?, ?)\n",
        "    \"\"\"\n",
        "    for stock in stock_data:\n",
        "        values = (\n",
        "            stock['date'],\n",
        "            stock['open'],\n",
        "            stock['high'],\n",
        "            stock['low'],\n",
        "            stock['close'],\n",
        "            stock['volume']\n",
        "        )\n",
        "        cursor.execute(insert_query, values)\n",
        "\n",
        "    # Commit the changes and close the connection\n",
        "    conn.commit()\n",
        "    cursor.close()\n",
        "    conn.close()\n",
        "\n",
        "# Example usage\n",
        "#csv_file_path = '/Tesla_stock_Price.csv'\n",
        "csv_file_path = '/ADANIPORTS.csv'  # Replace with your file path\n",
        "stocks = get_stock_data(csv_file_path)\n",
        "\n",
        "# Save the stock data to SQLite database\n",
        "db_file_path = 'stock_data.db'  # Replace with your desired database file path\n",
        "save_stock_data_to_sqlite(stocks, db_file_path)"
      ],
      "metadata": {
        "id": "cUsUpr5oDkBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### grabbing data from database"
      ],
      "metadata": {
        "id": "uk864RIaBAYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install prettytable"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZK6TlMccWte",
        "outputId": "ef720767-c924-49c6-b39c-2a4a061c3589"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.10/dist-packages (0.7.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "\n",
        "def save_stock_data_to_sqlite(stock_data, db_file_path):\n",
        "    # Making connection to the database file\n",
        "    conn = sqlite3.connect(db_file_path)\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    # Create a table if it doesn't exist\n",
        "    cursor.execute('''\n",
        "        CREATE TABLE IF NOT EXISTS stock_data (\n",
        "            symbol TEXT,\n",
        "            date TEXT,\n",
        "            open REAL,\n",
        "            high REAL,\n",
        "            low REAL,\n",
        "            close REAL,\n",
        "            volume INTEGER\n",
        "        )\n",
        "    ''')\n",
        "\n",
        "    # Insert stock data into the table\n",
        "    for row in stock_data:\n",
        "        symbol = row['symbol']\n",
        "        date = row['date']\n",
        "        open_price = row['open']\n",
        "        high_price = row['high']\n",
        "        low_price = row['low']\n",
        "        close_price = row['close']\n",
        "        volume = row['volume']\n",
        "\n",
        "        cursor.execute('''\n",
        "            INSERT INTO stock_data (symbol, date, open, high, low, close, volume)\n",
        "            VALUES (?, ?, ?, ?, ?, ?, ?)\n",
        "        ''', (symbol, date, open_price, high_price, low_price, close_price, volume))\n",
        "\n",
        "    # Commit the changes and close the connection\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "# Usage example:\n",
        "data = [\n",
        "    {\n",
        "        'symbol': 'AAPL',\n",
        "        'date': '2023-05-23',\n",
        "        'open': 125.0,\n",
        "        'high': 130.0,\n",
        "        'low': 124.5,\n",
        "        'close': 128.75,\n",
        "        'volume': 1000000\n",
        "    },\n",
        "    {\n",
        "        'symbol': 'ADANIPORTS',\n",
        "        'date': '2023-05-23',\n",
        "        'open': 125.0,\n",
        "        'high': 130.0,\n",
        "        'low': 124.5,\n",
        "        'close': 128.75,\n",
        "        'volume': 1000000\n",
        "    },\n",
        "    {\n",
        "        'symbol': 'GOOG',\n",
        "        'date': '2023-05-23',\n",
        "        'open': 125.0,\n",
        "        'high': 130.0,\n",
        "        'low': 124.5,\n",
        "        'close': 128.75,\n",
        "        'volume': 1000000\n",
        "    },\n",
        "    {\n",
        "        'symbol': 'MSFT',\n",
        "        'date': '2023-05-23',\n",
        "        'open': 125.0,\n",
        "        'high': 130.0,\n",
        "        'low': 124.5,\n",
        "        'close': 128.75,\n",
        "        'volume': 1000000\n",
        "    },\n",
        "    {\n",
        "        'symbol': 'Tesla_stock_Price',\n",
        "        'date': '2023-05-23',\n",
        "        'open': 125.0,\n",
        "        'high': 130.0,\n",
        "        'low': 124.5,\n",
        "        'close': 128.75,\n",
        "        'volume': 1000000\n",
        "    },\n",
        "    {\n",
        "        'symbol': 'Bitcoin',\n",
        "        'date': '2023-05-23',\n",
        "        'open': 125.0,\n",
        "        'high': 130.0,\n",
        "        'low': 124.5,\n",
        "        'close': 128.75,\n",
        "        'volume': 1000000\n",
        "    },\n",
        "    {\n",
        "        'symbol': 'AMZN_data',\n",
        "        'date': '2023-05-23',\n",
        "        'open': 125.0,\n",
        "        'high': 130.0,\n",
        "        'low': 124.5,\n",
        "        'close': 128.75,\n",
        "        'volume': 1000000\n",
        "    },\n",
        "    {\n",
        "        'symbol': 'all_stocks_5yr',\n",
        "        'date': '2023-05-23',\n",
        "        'open': 125.0,\n",
        "        'high': 130.0,\n",
        "        'low': 124.5,\n",
        "        'close': 128.75,\n",
        "        'volume': 1000000\n",
        "    },\n",
        "    {\n",
        "        'symbol': 'nasdaqtraded_full',\n",
        "        'date': '2023-05-23',\n",
        "        'open': 125.0,\n",
        "        'high': 130.0,\n",
        "        'low': 124.5,\n",
        "        'close': 128.75,\n",
        "        'volume': 1000000\n",
        "    },\n",
        "    {\n",
        "        'symbol': 'Dow Jones Industrial Average Historical Data',\n",
        "        'date': '2023-05-23',\n",
        "        'open': 125.0,\n",
        "        'high': 130.0,\n",
        "        'low': 124.5,\n",
        "        'close': 128.75,\n",
        "        'volume': 1000000\n",
        "    }\n",
        "]\n",
        "\n",
        "db_file_path = '/content/stock_data.db'\n",
        "save_stock_data_to_sqlite(data, db_file_path)\n"
      ],
      "metadata": {
        "id": "g_J3Wo4abADW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "\n",
        "def retrieve_stock_data_from_sqlite(db_file_path):\n",
        "    # Establish a connection to the database file\n",
        "    conn = sqlite3.connect('/content/stock_data.db')\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    # Retrieve data from the stock_data table\n",
        "    cursor.execute(\"SELECT * FROM stock_data\")\n",
        "    data = cursor.fetchall()\n",
        "\n",
        "    # Print the retrieved data\n",
        "    for row in data:\n",
        "        print(row)\n",
        "\n",
        "    # Close the connection\n",
        "    conn.close()\n",
        "\n",
        "# Provide the correct database file path\n",
        "db_file_path = '/content/stock_data.db'\n",
        "retrieve_stock_data_from_sqlite(db_file_path)"
      ],
      "metadata": {
        "id": "EluMr23UG2Kg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import sqlite3\n",
        "\n",
        "def scrape_news_articles(url, news_source):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        articles = soup.find_all('article')\n",
        "\n",
        "        news_data = []\n",
        "        for article in articles:\n",
        "            title = article.find('h2').text.strip()\n",
        "            summary = article.find('p').text.strip()\n",
        "            link = article.find('a')['href']\n",
        "            news_data.append({'title': title, 'summary': summary, 'link': link, 'source': news_source})\n",
        "\n",
        "        return news_data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Error occured during scrapping\", e)\n",
        "        return []\n",
        "\n",
        "cnn_url = 'https://edition.cnn.com/'\n",
        "cnbc_url = 'https://www.cnbc.com/world/?region=world'\n",
        "borse_url = 'https://www.boerse.de/'\n",
        "\n",
        "# Scrape news articles from CNN\n",
        "cnn_articles = scrape_news_articles(cnn_url, 'CNN')\n",
        "\n",
        "# Scrape news articles from CNBC\n",
        "cnbc_articles = scrape_news_articles(cnbc_url, 'CNBC')\n",
        "\n",
        "# Scrape news articles from Börse\n",
        "borse_articles = scrape_news_articles(borse_url, 'Börse')\n",
        "\n",
        "# Merge all the scraped articles into a single list\n",
        "all_articles = cnn_articles + cnbc_articles + borse_articles\n",
        "\n",
        "# Save news articles to SQLite database\n",
        "def save_articles_to_sqlite(articles, db_file_path):\n",
        "    try:\n",
        "        conn = sqlite3.connect('/content/news_articles.db')\n",
        "        cursor = conn.cursor()\n",
        "\n",
        "        cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS news_articles(\n",
        "              id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "              title TEXT,\n",
        "              summary TEXT,\n",
        "              link TEXT,\n",
        "              source TEXT\n",
        "            )\n",
        "         ''')\n",
        "\n",
        "        for article in articles:\n",
        "            title = article['title']\n",
        "            summary = article['summary']\n",
        "            link = article['link']\n",
        "            source = article['source']\n",
        "\n",
        "            cursor.execute('''  #cursor to itearte over rows of data\n",
        "                INSERT INTO news_articles (title, summary, link, source)\n",
        "                VALUES (?, ?, ?, ?)\n",
        "            ''', (title, summary, link, source))\n",
        "\n",
        "        conn.commit()\n",
        "        conn.close()\n",
        "    except Exception as e:\n",
        "          print(\"Error occured during data insertion:\", e)\n",
        "\n",
        "\n",
        "# Provide the correct database file path\n",
        "db_file_path = '/content/news_articles.db'\n",
        "save_articles_to_sqlite(all_articles, db_file_path)\n",
        "\n",
        "def save_articles_to_sqlite(articles, db_file_path):\n",
        "    try:\n",
        "        conn = sqlite3.connect(db_file_path)\n",
        "        cursor = conn.cursor()\n",
        "\n",
        "        cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS news_articles(\n",
        "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                title TEXT,\n",
        "                summary TEXT,\n",
        "                link TEXT,\n",
        "                source TEXT\n",
        "            )\n",
        "        ''')\n",
        "\n",
        "        for article in articles:\n",
        "            id = article['id']\n",
        "            title = article['title']\n",
        "            summary = article['summary']\n",
        "            link = article['link']\n",
        "            source = article['source']\n",
        "\n",
        "            cursor.execute('''\n",
        "                INSERT INTO news_articles (title, summary, link, source)\n",
        "                VALUES (?, ?, ?, ?)\n",
        "            ''', (title, summary, link, source))\n",
        "\n",
        "        conn.commit()\n",
        "        conn.close()\n",
        "    except Exception as e:\n",
        "        print(\"Error occurred during data insertion:\", e)\n",
        "\n",
        "import sqlite3\n",
        "\n",
        "def retrieve_news_articles_from_sqlite(db_file_path):\n",
        "    # Establish a connection to the database file\n",
        "    try:\n",
        "\n",
        "        conn = sqlite3.connect('/content/news_articles.db')\n",
        "        cursor = conn.cursor()\n",
        "\n",
        "        # Retrieve data from the stock_data table\n",
        "        cursor.execute(\"SELECT * FROM news_articles\")\n",
        "        data = cursor.fetchall()\n",
        "\n",
        "        # Print the retrieved data\n",
        "        for row in data:\n",
        "            print(row)\n",
        "\n",
        "        # Close the connection\n",
        "        conn.close()\n",
        "    except Exception as e:\n",
        "        print(\"error has occured during dtaa retrieval\")\n",
        "\n",
        "# Provide the correct database file path\n",
        "db_file_path = '/content/news_articles.db'\n",
        "retrieve_news_articles_from_sqlite(db_file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxsb3hiGHEz4",
        "outputId": "b5f948ca-da45-41e1-9163-83492ca3a7d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error occured during data insertion: database disk image is malformed\n",
            "error has occured during dtaa retrieval\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "import openai\n",
        "\n",
        "# Set up OpenAI API credentials\n",
        "openai.api_key = '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'\n",
        "\n",
        "# Function to retrieve data from SQLite database\n",
        "def retrieve_data_from_sqlite(db_file_path):\n",
        "    try:\n",
        "        conn = sqlite3.connect(db_file_path)\n",
        "        cursor = conn.cursor()\n",
        "\n",
        "        # Retrieve data from the database table\n",
        "        cursor.execute(\"SELECT * title FROM articles\")\n",
        "        data = cursor.fetchall()\n",
        "\n",
        "        # Close the database connection\n",
        "        conn.close()\n",
        "\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(\"Error occurred during data retrieval:\", e)\n",
        "        return []\n",
        "\n",
        "# Function to generate responses using ChatGPT\n",
        "def generate_responses(data):\n",
        "    for row in data:\n",
        "        article_title = row[0]\n",
        "\n",
        "        # Provide the article title as the input prompt\n",
        "        prompt = f\"Article Title: {article_title}\\n\"\n",
        "\n",
        "        # Generate response from ChatGPT\n",
        "        response = openai.Completion.create(\n",
        "            engine='davinci-codex',\n",
        "            prompt=prompt,\n",
        "            max_tokens=50,\n",
        "            temperature=0.7,\n",
        "            n=1,\n",
        "            stop=None,\n",
        "            timeout=10\n",
        "        )\n",
        "\n",
        "        # Extract and print the generated response\n",
        "        generated_text = response.choices[0].text.strip()\n",
        "        print(f\"Generated Response for '{article_title}': {generated_text}\")\n",
        "\n",
        "# Provide the correct database file path\n",
        "db_file_path = '/content/news_articles.db'\n",
        "\n",
        "# Retrieve data from SQLite database\n",
        "data = retrieve_data_from_sqlite(db_file_path)\n",
        "\n",
        "# Generate responses using ChatGPT\n",
        "generate_responses(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7uLIy0Wr7rkL",
        "outputId": "7356bbbd-9227-4b27-a82a-bc6156d9daa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error occurred during data retrieval: near \"title\": syntax error\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "import openai\n",
        "\n",
        "# Establish a connection to the SQLite database\n",
        "def connect_to_database(db_file_path):\n",
        "    try:\n",
        "        conn = sqlite3.connect(db_file_path)\n",
        "        return conn\n",
        "    except Exception as e:\n",
        "        print(\"Error occurred during database connection:\", e)\n",
        "        return None\n",
        "\n",
        "# Retrieve news articles from the SQLite database\n",
        "def retrieve_news_articles_from_sqlite(db_file_path):\n",
        "    try:\n",
        "        cursor = conn.cursor()\n",
        "        #cursor.execute(\"SELECT * FROM stock_data\")\n",
        "        cursor.execute(\"SELECT * FROM news_articles\")\n",
        "        articles = cursor.fetchall()\n",
        "        return articles\n",
        "    except Exception as e:\n",
        "        print(\"Error occurred during data retrieval:\", e)\n",
        "        return []\n",
        "\n",
        "# Generate a response using ChatGPT\n",
        "def generate_response(input_text):\n",
        "    openai.api_key = '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'  # Replace with your OpenAI API key\n",
        "    response = openai.Completion.create(\n",
        "        engine='text-davinci-003',\n",
        "        prompt=input_text,\n",
        "        max_tokens=100,\n",
        "        n=1,\n",
        "        stop=None,\n",
        "        temperature=0.7\n",
        "    )\n",
        "    return response.choices[0].text.strip()\n",
        "\n",
        "# Provide the correct database file path\n",
        "db_file_path = '/content/stock_data.db'\n",
        "#db_file_path = '/content/news_articles.db'\n",
        "conn = connect_to_database(db_file_path)\n",
        "\n",
        "# Retrieve news articles from the database\n",
        "articles = retrieve_news_articles_from_sqlite(conn)\n",
        "\n",
        "# Iterate over the articles and generate responses\n",
        "for article in articles:\n",
        "    # Prepare input text for the ChatGPT model\n",
        "    input_text = article[1]  # Assuming the title is in the second column (index 1)\n",
        "\n",
        "    # Generate a response using ChatGPT\n",
        "    response = generate_response(input_text)\n",
        "\n",
        "    # Print the generated response\n",
        "    print(f\"Generated Response for '{input_text}': {response}\")\n",
        "\n",
        "# Close the database connection\n",
        "conn.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IDKuy1-0Rtq",
        "outputId": "baa03a4b-eee4-4cf2-c512-685210af540d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error occurred during data retrieval: no such table: news_articles\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(cnn_articles)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tYwcvf_Kr4R",
        "outputId": "44f75c9b-0a66-406c-a18a-0e9d9d3ddac0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import sqlite3\n",
        "\n",
        "def scrape_news_articles(url, news_source):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    articles = soup.find_all('article')\n",
        "\n",
        "    news_data = []\n",
        "    for index, article in enumerate(articles):\n",
        "        title = article.find('h2').text.strip()\n",
        "        summary = article.find('p').text.strip()\n",
        "        link = article.find('a')['href']\n",
        "        news_data.append({'id': index+1, 'title': title, 'summary': summary, 'link': link, 'source': news_source})\n",
        "\n",
        "    return news_data\n",
        "\n",
        "cnn_url = 'https://edition.cnn.com/'\n",
        "cnbc_url = 'https://www.cnbc.com/world/?region=world'\n",
        "borse_url = 'https://www.boerse.de/'\n",
        "\n",
        "# Scrape news articles from CNN\n",
        "cnn_articles = scrape_news_articles(cnn_url, 'CNN')\n",
        "print(\"cnn_articles:\",cnn_articles)\n",
        "# Scrape news articles from CNBC\n",
        "cnbc_articles = scrape_news_articles(cnbc_url, 'CNBC')\n",
        "print(\"cnbc_articles:\", cnbc_articles)\n",
        "\n",
        "# Scrape news articles from Börse\n",
        "borse_articles = scrape_news_articles(borse_url, 'Börse')\n",
        "print(\"börse_articles:\", borse_articles)\n",
        "# Merge all the scraped articles into a single list\n",
        "all_articles = cnn_articles + cnbc_articles + borse_articles\n",
        "\n",
        "print(\"all_articles:\", all_articles)\n",
        "# Save news articles to SQLite database\n",
        "def save_articles_to_sqlite(articles, db_file_path):\n",
        "    try:\n",
        "        conn = sqlite3.connect(db_file_path)\n",
        "        print(conn)\n",
        "        cursor = conn.cursor()\n",
        "\n",
        "        cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS news_articles(\n",
        "              id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "              title TEXT,\n",
        "              summary TEXT,\n",
        "              link TEXT,\n",
        "              source TEXT\n",
        "            )\n",
        "        ''')\n",
        "\n",
        "        for article in articles:\n",
        "            #id = article['id']\n",
        "            title = article['title']\n",
        "            summary = article['summary']\n",
        "            link = article['link']\n",
        "            source = article['source']\n",
        "\n",
        "            cursor.execute('''\n",
        "                INSERT INTO news_articles (title, summary, link, source)\n",
        "                VALUES (?, ?, ?, ?)\n",
        "                ''', (title, summary, link, source))\n",
        "\n",
        "        conn.commit()\n",
        "        conn.close()\n",
        "        print(\"Data inserted sucessfully.\")\n",
        "    except Exception as e:\n",
        "        print(\"Error occured during data insertion\")\n",
        "\n",
        "# Provide the correct database file path\n",
        "db_file_path = '/content/news_articles.db'\n",
        "save_articles_to_sqlite(all_articles, db_file_path)\n",
        "\n",
        "print(all_articles)\n",
        "\n",
        "import sqlite3\n",
        "\n",
        "def retrieve_stock_data_from_sqlite(db_file_path):\n",
        "    # Establish a connection to the database file\n",
        "    try:\n",
        "        conn = sqlite3.connect(db_file_path)\n",
        "        cursor = conn.cursor()\n",
        "\n",
        "        # Retrieve data from the stock_data table\n",
        "        cursor.execute(\"SELECT * FROM news_articles\")\n",
        "        data = cursor.fetchall()\n",
        "\n",
        "\n",
        "        # Print the retrieved data\n",
        "        for row in data:\n",
        "            print(row)\n",
        "\n",
        "\n",
        "        # Close the connection\n",
        "        conn.close()\n",
        "    except Exception as e:\n",
        "        print(\"error occured during data retrieval\")\n",
        "\n",
        "# Provide the correct database file path\n",
        "db_file_path = \"/content/news_articles.db\"\n",
        "retrieve_stock_data_from_sqlite(db_file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oFCuyUCPGTD",
        "outputId": "d40d929f-f544-4cca-fed6-ed7741cf7320"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cnn_articles: []\n",
            "cnbc_articles: []\n",
            "börse_articles: []\n",
            "all_articles: []\n",
            "<sqlite3.Connection object at 0x7f8f50406c40>\n",
            "Data inserted sucessfully.\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_articles_to_sqlite(articles, db_file_path):\n",
        "    try:\n",
        "        conn = sqlite3.connect(db_file_path)\n",
        "        print(conn)\n",
        "        cursor = conn.cursor()\n",
        "\n",
        "        cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS news_articles(\n",
        "              id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "              title TEXT,\n",
        "              summary TEXT,\n",
        "              link TEXT,\n",
        "              source TEXT\n",
        "            )\n",
        "        ''')\n",
        "\n",
        "        for article in articles:\n",
        "            #id = article['id']\n",
        "            title = article['title']\n",
        "            summary = article['summary']\n",
        "            link = article['link']\n",
        "            source = article['source']\n",
        "\n",
        "            cursor.execute('''\n",
        "                INSERT INTO news_articles (title, summary, link, source)\n",
        "                VALUES (?, ?, ?, ?)\n",
        "                ''', (title, summary, link, source))\n",
        "\n",
        "        conn.commit()\n",
        "        conn.close()\n",
        "        print(\"Data inserted sucessfully.\")\n",
        "    except Exception as e:\n",
        "        print(\"Error occured during data insertion\")\n",
        "\n",
        "# Provide the correct database file path\n",
        "db_file_path = '/content/news_articles.db'\n",
        "save_articles_to_sqlite(all_articles, db_file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7V2qq86o9ua",
        "outputId": "42bfb575-9449-4b19-c26d-fde0ee0fea91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<sqlite3.Connection object at 0x7f8f50404240>\n",
            "Data inserted sucessfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "\n",
        "def check_table_structure(db_file_path, table_name):\n",
        "    conn = sqlite3.connect(db_file_path)\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    cursor.execute(f\"PRAGMA table_info({table_name})\")\n",
        "    columns = cursor.fetchall()\n",
        "\n",
        "    print(f\"Table: {table_name}\")\n",
        "    for column in columns:\n",
        "        print(column)\n",
        "\n",
        "    conn.close()\n",
        "\n",
        "# Provide the correct database file path and table name\n",
        "db_file_path = '/content/news_articles.db'\n",
        "table_name = 'news_articles'\n",
        "\n",
        "# Call the function to check the table structure\n",
        "check_table_structure(db_file_path, table_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aK08C2TwcPQT",
        "outputId": "0709ac0d-2372-4892-e94d-83b50fad7ebb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Table: news_articles\n",
            "(0, 'id', 'INTEGER', 0, None, 1)\n",
            "(1, 'title', 'TEXT', 0, None, 0)\n",
            "(2, 'summary', 'TEXT', 0, None, 0)\n",
            "(3, 'link', 'TEXT', 0, None, 0)\n",
            "(4, 'source', 'TEXT', 0, None, 0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tabulate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8K3qYfOYVCr",
        "outputId": "2a6ce626-a903-4283-9fd0-966c14cd3a73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (0.8.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "\n",
        "def retrieve_news_articles_from_sqlite(db_file_path):\n",
        "    # Establish a connection to the database file\n",
        "    try:\n",
        "        conn = sqlite3.connect(db_file_path)\n",
        "        cursor = conn.cursor()\n",
        "\n",
        "        # Retrieve data from the stock_data table\n",
        "        cursor.execute(\"SELECT * FROM news_articles\")\n",
        "        data = cursor.fetchall()\n",
        "\n",
        "\n",
        "        # Print the retrieved data\n",
        "        for row in data:\n",
        "            print(row)\n",
        "\n",
        "\n",
        "        # Close the connection\n",
        "        conn.close()\n",
        "    except:\n",
        "        print(\"error occured during adta retrieval\")\n",
        "\n",
        "# Provide the correct database file path\n",
        "db_file_path = '/content/news_articles.db'\n",
        "retrieve_news_articles_from_sqlite(db_file_path)\n",
        "\n",
        "\n",
        "#print(\"hello word\")"
      ],
      "metadata": {
        "id": "ajo8CuzJJvjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "from tabulate import tabulate\n",
        "\n",
        "def retrieve_news_articles_from_sqlite(db_file_path):\n",
        "    try:\n",
        "        conn = sqlite3.connect(db_file_path)\n",
        "        cursor = conn.cursor()\n",
        "\n",
        "        cursor.execute(\"SELECT * FROM news_articles\")\n",
        "        data = cursor.fetchall()\n",
        "\n",
        "        headers = ['ID', 'Title', 'Summary', 'Link', 'Source']\n",
        "        table = tabulate(data, headers=headers, tablefmt='psql')\n",
        "        print(table)\n",
        "\n",
        "        conn.close()\n",
        "    except Exception as e:\n",
        "        print(\"Error occurred during data retrieval:\", e)\n",
        "\n",
        "# Provide the correct database file path\n",
        "db_file_path = '/content/news_articles.db'\n",
        "retrieve_news_articles_from_sqlite(db_file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHv66dy4ZK2a",
        "outputId": "cdc48f52-f49f-418e-af86-5cf359b612ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---------+-----------+--------+----------+\n",
            "| ID   | Title   | Summary   | Link   | Source   |\n",
            "|------+---------+-----------+--------+----------|\n",
            "+------+---------+-----------+--------+----------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "conn = sqlite3.connect('/content/news_articles.db')\n",
        "cursor= conn.cursor()\n",
        "query = \"select count(*) from news_articles\"\n",
        "cursor.execute(query)\n",
        "result = cursor.fetchone()\n",
        "\n",
        "if result[0] > 0:\n",
        "    print(\"Data is present in the table\")\n",
        "else:\n",
        "    print(\"No Data is present in the table\")\n",
        "\n",
        "cursor.close()\n",
        "conn.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EB39DwJNDDpS",
        "outputId": "6b6da328-79b5-416a-8c5a-7d99e87c96bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No Data is present in the table\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "\n",
        "def retrieve_stock_data_from_sqlite(db_file_path):\n",
        "    # Establish a connection to the database file\n",
        "    try:\n",
        "        conn = sqlite3.connect(db_file_path)\n",
        "        cursor = conn.cursor()\n",
        "\n",
        "        # Retrieve data from the stock_data table\n",
        "        cursor.execute(\"SELECT * FROM news_articles\")\n",
        "        data = cursor.fetchall()\n",
        "\n",
        "\n",
        "        # Print the retrieved data\n",
        "        for row in data:\n",
        "            print(row)\n",
        "\n",
        "\n",
        "        # Close the connection\n",
        "        conn.close()\n",
        "    except:\n",
        "        print(\"error occured during adta retrieval\")\n",
        "\n",
        "# Provide the correct database file path\n",
        "db_file_path = '/content/news_articles.db'\n",
        "retrieve_stock_data_from_sqlite(db_file_path)"
      ],
      "metadata": {
        "id": "uwTUcgrfYneU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "TFrfbu1UnTmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import openai\n",
        "\n",
        "# Define your API key and endpoint\n",
        "chatgptAPI_KEY = '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'\n",
        "chatgptAPI_url = 'https://openai.com/blog/chatgpt'\n",
        "\n",
        "def generate_response(input_text):\n",
        "    openai.api_key = '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'  # Replace with your OpenAI API key\n",
        "    response = openai.Completion.create(\n",
        "        engine='text-davinci-003',\n",
        "        prompt=input_text,\n",
        "        max_tokens=100,\n",
        "        n=1,\n",
        "        stop=None,\n",
        "        temperature=0.7\n",
        "    )\n",
        "    return response.choices[0].text.strip()\n",
        "\n",
        "conn = sqlite3.connect('news_articles.db')\n",
        "cursor = conn.cursor()\n",
        "\n"
      ],
      "metadata": {
        "id": "317iAjBFhdDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import openai\n",
        "\n",
        "# Define your API key and endpoint\n",
        "chatgptAPI_KEY = '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'\n",
        "chatgptAPI_url = 'https://openai.com/blog/chatgpt'\n",
        "\n",
        "def generate_response(input_text):\n",
        "    openai.api_key = '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'  # Replace with your OpenAI API key\n",
        "    response = openai.Completion.create(\n",
        "        engine='text-davinci-003',\n",
        "        prompt=input_text,\n",
        "        max_tokens=100,\n",
        "        n=1,\n",
        "        stop=None,\n",
        "        temperature=0.7\n",
        "    )\n",
        "    return response.choices[0].text.strip()\n",
        "\n",
        "conn = sqlite3.connect('stock_data.db')\n",
        "cursor = conn.cursor()\n",
        "\n",
        "cursor.execute('select * from stock_data')\n",
        "rows = cursor.fetchall()\n",
        "\n",
        "# function to get table columns from sqlite database\n",
        "def get_table_columns(stock_data):\n",
        "  cursor.execute(\"PRAGMA table_info({})\".format(stock_data))\n",
        "  columns = cursor.fetchall()\n",
        "  print(columns)\n",
        "  return[]\n",
        "\n"
      ],
      "metadata": {
        "id": "lDKs2l0O6dZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "\n",
        "conn = sqlite3.connect('news_articles.db')\n",
        "cursor = conn.cursor()\n",
        "\n",
        "cursor.execute('SELECT * FROM news_articles')\n",
        "rows = cursor.fetchall()\n",
        "\n",
        "for row in rows:\n",
        "    print(row)\n",
        "\n",
        "conn.close()"
      ],
      "metadata": {
        "id": "tLE3OVbE7JMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "\n",
        "try:\n",
        "    conn = sqlite3.connect('news_articles.db')\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    cursor.execute('SELECT * FROM news_articles')\n",
        "    rows = cursor.fetchall()\n",
        "\n",
        "    for row in rows:\n",
        "        print(row)\n",
        "\n",
        "except sqlite3.Error as e:\n",
        "    print(\"Error connecting to database:\", e)\n",
        "\n",
        "finally:\n",
        "    if conn:\n",
        "        conn.close()"
      ],
      "metadata": {
        "id": "2meeBIO17yyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "\n",
        "connection = sqlite3.connect(\"stock_data.db\")\n",
        "cursor = connection.cursor()\n",
        "\n",
        "select_query = \"SELECT * FROM stock_data\"\n",
        "cursor.execute(select_query)\n",
        "\n",
        "rows = cursor.fetchall()\n",
        "\n",
        "for row in rows:\n",
        "    print(row)\n",
        "\n",
        "cursor.close()\n",
        "connection.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUu2bcl3t5db",
        "outputId": "98c77f75-1b3e-4a01-f645-b87c081a62d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('AAPL', '2023-05-23', 125.0, 130.0, 124.5, 128.75, 1000000)\n",
            "('ADANIPORTS', '2023-05-23', 125.0, 130.0, 124.5, 128.75, 1000000)\n",
            "('GOOG', '2023-05-23', 125.0, 130.0, 124.5, 128.75, 1000000)\n",
            "('MSFT', '2023-05-23', 125.0, 130.0, 124.5, 128.75, 1000000)\n",
            "('Tesla_stock_Price', '2023-05-23', 125.0, 130.0, 124.5, 128.75, 1000000)\n",
            "('Bitcoin', '2023-05-23', 125.0, 130.0, 124.5, 128.75, 1000000)\n",
            "('AMZN_data', '2023-05-23', 125.0, 130.0, 124.5, 128.75, 1000000)\n",
            "('all_stocks_5yr', '2023-05-23', 125.0, 130.0, 124.5, 128.75, 1000000)\n",
            "('nasdaqtraded_full', '2023-05-23', 125.0, 130.0, 124.5, 128.75, 1000000)\n",
            "('Dow Jones Industrial Average Historical Data', '2023-05-23', 125.0, 130.0, 124.5, 128.75, 1000000)\n",
            "('AAPL', '2023-05-23', 125.0, 130.0, 124.5, 128.75, 1000000)\n",
            "('ADANIPORTS', '2023-05-23', 125.0, 130.0, 124.5, 128.75, 1000000)\n",
            "('GOOG', '2023-05-23', 125.0, 130.0, 124.5, 128.75, 1000000)\n",
            "('MSFT', '2023-05-23', 125.0, 130.0, 124.5, 128.75, 1000000)\n",
            "('Tesla_stock_Price', '2023-05-23', 125.0, 130.0, 124.5, 128.75, 1000000)\n",
            "('Bitcoin', '2023-05-23', 125.0, 130.0, 124.5, 128.75, 1000000)\n",
            "('AMZN_data', '2023-05-23', 125.0, 130.0, 124.5, 128.75, 1000000)\n",
            "('all_stocks_5yr', '2023-05-23', 125.0, 130.0, 124.5, 128.75, 1000000)\n",
            "('nasdaqtraded_full', '2023-05-23', 125.0, 130.0, 124.5, 128.75, 1000000)\n",
            "('Dow Jones Industrial Average Historical Data', '2023-05-23', 125.0, 130.0, 124.5, 128.75, 1000000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E8ocPWG26Y8u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}